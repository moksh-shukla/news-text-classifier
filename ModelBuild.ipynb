{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModelBuild.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6lrDkYOLgTH",
        "outputId": "bc535819-a493-4d85-a7b4-fe1adb4a1a62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "import requests\n",
        "import pickle\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('/content/drive/MyDrive/SNF_IITK/model.csv')"
      ],
      "metadata": {
        "id": "uE3MoVqVLsft"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## functions to clean the news text\n",
        "def removeTag(text):\n",
        "    remove = re.compile(r'')\n",
        "    return re.sub(remove, '', text)\n",
        "\n",
        "#removes special characters\n",
        "def removeSpec_char(text):\n",
        "    cleanString = text.replace(\"\\n\", \" \")\n",
        "    return cleanString\n",
        "\n",
        "def lowerCase(text):\n",
        "    return text.lower()\n",
        "\n",
        "#removes punctuation\n",
        "def remove_punc(text):\n",
        "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "    new_words = tokenizer.tokenize(text)\n",
        "    return new_words\n",
        "\n",
        "#removes stop words\n",
        "def removeStop(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    return [x for x in words if x not in stop_words]\n",
        "\n",
        "# lemmatizes the words\n",
        "def lemmaWord(text):\n",
        "    wordnet = WordNetLemmatizer()\n",
        "    return \" \".join([wordnet.lemmatize(word) for word in text])"
      ],
      "metadata": {
        "id": "wrCl8mdgv6QW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['news'] = dataset['news'].apply(removeTag)\n",
        "dataset['news'] = dataset['news'].apply(removeSpec_char)\n",
        "dataset['news'] = dataset['news'].apply(lowerCase)\n",
        "dataset['news'] = dataset['news'].apply(remove_punc)\n",
        "dataset['news'] = dataset['news'].apply(lemmaWord)\n",
        "dataset['news'] = dataset['news'].apply(removeStop)\n",
        "dataset['news'] = dataset['news'].apply(lemmaWord)"
      ],
      "metadata": {
        "id": "NW7DDshLwdIx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing \n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "dataset['label']= label_encoder.fit_transform(dataset['subcateg'])\n",
        "dataset['label'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM9qUZMgQPgh",
        "outputId": "49f93d94-fd3b-4576-e58a-04ff0fbde9ef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([12,  1, 10,  3,  6,  4,  8,  0,  2,  7,  9, 11,  5])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array(dataset.iloc[:,0].values)\n",
        "y = np.array(dataset.label.values)\n",
        "cv = CountVectorizer(max_features = 5000)\n",
        "x = cv.fit_transform(dataset.news).toarray()\n",
        "print(\"X.shape = \",x.shape)\n",
        "print(\"y.shape = \",y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srZWZUi_QkKD",
        "outputId": "c81a38dd-175c-4ec4-896c-0a6915b54ad0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape =  (23672, 5000)\n",
            "y.shape =  (23672,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 0, shuffle = True)\n",
        "print(len(x_train))\n",
        "print(len(x_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaT2AS0hQWse",
        "outputId": "41d90a71-9c34-4c5d-df75-d3aa9aa444a5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21304\n",
            "2368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#One v Rest classifier with random forest decision function\n",
        "rf = RandomForestClassifier(n_estimators=100 ,criterion='entropy' , random_state=0)\n",
        "model = OneVsRestClassifier(rf)\n",
        "model.fit(x_train, y_train)\n",
        "y_pred = model.predict(x_test)"
      ],
      "metadata": {
        "id": "urkeKoMqcklu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = round(accuracy_score(y_test, y_pred) * 100, 2)\n",
        "# Get precision, recall, f1 scores\n",
        "precision, recall, f1score, support = score(y_test, y_pred, average='micro')\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision : {precision}')\n",
        "print(f'Recall : {recall}')\n",
        "print(f'F1-score : {f1score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDIGakmV1NUw",
        "outputId": "91ad8299-0559-4693-8afa-c3c443603899"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 72.17\n",
            "Precision : 0.721706081081081\n",
            "Recall : 0.721706081081081\n",
            "F1-score : 0.721706081081081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'finalized_model.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "E3o-3DAwcigg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}